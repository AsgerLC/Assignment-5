---
title: "Assignment 5 - Meta-analysis of pitch in schizophrenia"
author: "Riccardo Fusaroli"
date: "3/7/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Building on the shoulders of giants: meta-analysis

######## Thingamajigs to be reported

- What does the evidence look like overall?
- HOw many papers report quantitative estimates?
- What percentage of the overall studies reviewed do they represent?
- What method have we used to analyze them?
- What is the mean effect size across studies for ze zwei features.

- Do the results align with the results we totally have from Assignment 3 (that we may have to make)
- If we somehow add our results to the metaanalysis, does it change?

- Is there heterogeneity between studies?
- Is there publication bias?
- Are any studies more influential than any others?


######### THINGS TO ASK RICCARDO ABOUT



- Is including a fixed effect of "task" what was meant by "context". Should we do any more with this model? Should we report the significant results, et cetera, et cetera.
- Is it a problem that the random effects for Feature 2 do not explain anything at all (boundary singular fit)
- What does the random effects mean?
- the hell the plots means???????????!1????
-  How do we interpret the summaries of rma with tasks? <- AND the resulting funky forest plots?
- How do we interpret the influencial studies analysis, and what should we do with it? 
- standard deviation of mean or mean of standard deviation (at trial level) when adding Assignment 3




## Questions to be answered
 
1. What is the current evidence for distinctive vocal patterns in schizophrenia? Report how many papers report quantitative estimates, comment on what percentage of the overall studies reviewed they represent (see PRISMA chart) your method to analyze them, the estimated effect size of the difference (mean effect size and standard error) and forest plots representing it. N.B. Only measures of pitch mean and pitch sd are required for the assignment. Feel free to ignore the rest (although pause behavior looks interesting, if you check my article).

2. Do the results match your own analysis from Assignment 3? If you add your results to the meta-analysis, do the estimated effect sizes change? Report the new estimates and the new forest plots.

3. Assess the quality of the literature: report and comment on heterogeneity of the studies (tau, I2), on publication bias (funnel plot), and on influential studies.

## Tips on the process to follow:

- Download the data on all published articles analyzing voice in schizophrenia and the prisma chart as reference of all articles found and reviewed
- Look through the dataset to find out which columns to use, and if there is any additional information written as comments (real world data is always messy!).
    * Hint: PITCH_F0M and PITCH_F0SD group of variables are what you need
    * Hint: Make sure you read the comments in the columns: `pitch_f0_variability`, `frequency`, `Title`,  `ACOUST_ANA_DESCR`, `DESCRIPTION`, and `COMMENTS`
- Following the procedure in the slides calculate effect size and standard error of the effect size per each study. N.B. we focus on pitch mean and pitch standard deviation.
 . first try using lmer (to connect to what you know of mixed effects models)
 . then use rma() (to get some juicy additional statistics)

- Build a forest plot of the results (forest(model))
 
- Go back to Assignment 3, add your own study to the data table, and re-run meta-analysis. Do the results change?

- Now look at the output of rma() and check tau and I2






```{r}
### PREPROCESSING THE DATA

## Loading packages
pacman::p_load(pacman, readxl, tidyverse, metafor, plyr)

dataset <- read_excel("Matrix_MetaAnalysis_Diagnosis_updated290719.xlsx")

# Chosing the right vaiables for feature 1: 
df_f1 <- dataset %>% filter(ArticleID, StudyID, SAMPLE_SIZE_HC, SAMPLE_SIZE_SZ, PITCH_F0_HC_M, PITCH_F0_SZ_M, PITCH_F0_HC_SD, PITCH_F0_SZ_SD) %>% select(ArticleID, StudyID, TYPE_OF_TASK, SAMPLE_SIZE_HC, SAMPLE_SIZE_SZ, PITCH_F0_HC_M, PITCH_F0_SZ_M, PITCH_F0_HC_SD, PITCH_F0_SZ_SD)

# Chosing the right vaiables for feature 1: 
df_f2 <- dataset %>% filter(ArticleID, StudyID, SAMPLE_SIZE_HC, SAMPLE_SIZE_SZ, PITCH_F0SD_HC_M, PITCH_F0SD_SZ_M, PITCH_F0SD_HC_SD, PITCH_F0SD_SZ_SD) %>% select(ArticleID, StudyID, TYPE_OF_TASK, SAMPLE_SIZE_HC, SAMPLE_SIZE_SZ, PITCH_F0SD_HC_M, PITCH_F0SD_SZ_M, PITCH_F0SD_HC_SD, PITCH_F0SD_SZ_SD)

# Changing the column names to something more manageable 
colnames(df_f1) <- c("articleID", "studyID","task", "n_hc", "n_sz", "f1_hc_mean", "f1_sz_mean", "f1_hc_sd", "f1_sz_sd")
colnames(df_f2) <- c("articleID", "studyID", "task","n_hc", "n_sz", "f2_hc_mean", "f2_sz_mean", "f2_hc_sd", "f2_sz_sd")

# Turning task into a factor
df_f1$task <- as.factor(df_f1$task)
df_f2$task <- as.factor(df_f2$task)

write.csv(df_f1, "F1 Dataframe.csv")
write.csv(df_f2, "F2 Dataframe.csv")

### ANALYSIS (According to Riccardo's Slides)

## First, an effect size / Cohen's d estimate is calculated for each study, and we don't want to do this manually (because the math is far too fancy). Instead, we use the metafor package to do so. IMPORTANTLY, this is done separately for pitch mean and pitch variability.

## metafor uses the escalc function to give us an SMD for each study. It names this variable "yi". Additionally, it provides for us a variance estimate for each study called "vi". "i" references each individual study.


# Effect size calculations for feature 1 (Participant's pitch mean)

es_f1 <- escalc('SMD',
                      n1i = n_hc,
                      n2i = n_sz,
                      m1i = f1_hc_mean, 
                      m2i = f1_sz_mean,
                      sd1i = f1_hc_sd, 
                      sd2i = f1_sz_sd,
                      data = df_f1)


# Effect size calculations for feature 2 (Participant's pitch variability)
es_f2 <- escalc('SMD',
                      n1i = n_hc,
                      n2i = n_sz,
                      m1i = f2_hc_mean, 
                      m2i = f2_sz_mean,
                      sd1i = f2_hc_sd, 
                      sd2i = f2_sz_sd,
                      data = df_f2)

## So, now we have two variables added to the two datasets called yi and vi. Huzzah.

## Now we want to do something with that yi variable, that Standardized Mean Difference. We can make an average between the studies, we can make a weighted average between the studies, or best of all, we can make a weighted average that takes into account the study heterogeneity (By including random intercepts for studies. Or something.) Don't think about it too hard.

# Thus, the basic pattern becomes:

mod_f1 <- lmerTest::lmer(yi ~ 1 + (1 | studyID), es_f1, weights = 1/vi, REML=F, control = lme4::lmerControl( check.nobs.vs.nlev ='ignore', check.nobs.vs.nRE = 'ignore'))
summary(mod_f1) # Alright, the estimated SMD is -0.2065.

# This is the *weighted average effect size* (with heterogeneity taken into account) of the difference in feature 1 (Pitch Mean) between Healthy Controls and Schizophrenics across studies. Phew.

# We may wish to include a fixed effect of type of task, to see if it changes anything:

task_f1 <- lmerTest::lmer(yi ~ 1 + task + (1 | studyID), es_f1, weights = 1/vi, REML=F, control = lme4::lmerControl( check.nobs.vs.nlev ='ignore', check.nobs.vs.nRE = 'ignore'))
summary(task_f1)
# Changing type of task does not significantly impact the SMD estimate. The SE for the estimates is unfortunately quite high, so we cannot hope to explain much variance between studies with referrence to task.
# As a result, we won't be using this particular model for anything else.

## Rinse repeat for F2:

mod_f2 <- lmerTest::lmer(yi ~ 1 + (1 | studyID), es_f2, weights = 1/vi, REML=F, control = lme4::lmerControl( check.nobs.vs.nlev ='ignore', check.nobs.vs.nRE = 'ignore'))
summary(mod_f2) # Alright, the estimated SMD is 0.2333. So, hold on to your horses here, this is the weighted average standardized mean difference (with random effects) of the differences in Standard Deviation between Healthy Controls and Schizophrenics... across studies.

# this is something we could ask Ricardo about.


# We may wish to include a fixed effect of type of task, to see if it changes anything:

task_f2 <- lmerTest::lmer(yi ~ 1 + task + (1 | studyID), es_f2, weights = 1/vi, REML=F, control = lme4::lmerControl( check.nobs.vs.nlev ='ignore', check.nobs.vs.nRE = 'ignore'))
summary(task_f2) # The standardized mean difference of Feature 2 changes significantly in a positive direction when moving from TASK_CONSTRUCTED to TASK_SOCIAL, in other words, this may mean that some of the heterogeneity between the studies (relating specifically to feature 2) is a product of the difference in task.

### - Ask Riccardo if this is what was meant by context and whether we should do any more with it.


# We may also wish to plot the results. Or maybe not.

plot(mod_f1) # I would like some aid in interpreting this.
plot(mod_f2) # THis plot is a bit weird with or without the random effects.

### Ask Riccardo for help in interpreting the plots + whether we should report them.

### Now, we can basically do all of the above using metafor's "rma" function.

rma_f1 <- rma(yi, vi, data = es_f1, slab = studyID)
summary(rma_f1) # Gives a somewhat different output. Scaled, perhaps? ### ASK RICCARDO
forest(rma_f1) # Nice forest plot of the metaanalysis! ...What does it show again?

#estimate      se     zval    pval    ci.lb   ci.ub 
# -0.1628  0.1554  -1.0476  0.2948  -0.4672  0.1417    

#tau^2 (estimated amount of total heterogeneity): 0.0712 (SE = 0.0908)
#tau (square root of estimated tau^2 value):      0.2668
#I^2 (total heterogeneity / total variability):   50.29%
#H^2 (total variability / sampling variability):  2.01

#Test for Heterogeneity:
#Q(df = 5) = 9.8472, p-val = 0.0797

# Now with task 
rma_f1_task <-rma(yi, vi, mods = cbind(task), data = es_f1, slab=studyID)
summary(rma_f1_task) # No significant effect of task. Consistent results. Yaaaay.
forest(rma_f1_task)

# For feature 2
rma_f2 <- rma(yi, vi, data = es_f2, slab = studyID)
summary(rma_f2)

# Tau^2 (estimated amount of total heterogeneity):
#tau (square root of estimated tau^2 value):             0.3040
#I^2 (residual heterogeneity / unaccounted variability): 55.34%
#H^2 (unaccounted variability / sampling variability):   2.24
#R^2 (amount of heterogeneity accounted for):            0.00%

#Test for Residual Heterogeneity:
#QE(df = 4) = 9.0272, p-val = 0.0604

#Test of Moderators (coefficient 2):
#QM(df = 1) = 0.5154, p-val = 0.4728

forest(rma_f2)# It do be lookethening like there doth be an influential study, yo.15 <- dat one.

# Now with task (again)
rma_task_f2 <-rma(yi, vi, mods = cbind(task), data = es_f2, slab = studyID)
summary(rma_task_f2)
#tau^2 (estimated amount of residual heterogeneity):     1.2275 (SE = 0.5120)
#tau (square root of estimated tau^2 value):             1.1079
#I^2 (residual heterogeneity / unaccounted variability): 95.16%
#H^2 (unaccounted variability / sampling variability):   20.68
#R^2 (amount of heterogeneity accounted for):            0.00%

#Test for Residual Heterogeneity:
#QE(df = 13) = 158.7379, p-val < .0001

#Test of Moderators (coefficient 2):
#QM(df = 1) = 0.7244, p-val = 0.3947


forest(rma_task_f2)

### WHAT WE DO WHEN WE DO QUALITY CHECKIN'.

## First of all: Study Heterogeneity. All studies present some individual variance (within study) due to people being different and such. Between-study variance shows up as a result of different effect sizes. So the question becomes:

# How much of the between-studies variance is reducible to the fact that the studies are uncertain (and aggregated estimates therefore will vary)? 

# You calculate this using Tau (A measure of overall variance between studies). An essential component of tau is Q, which is the ratio of observed variance to within-study variance.

# Based on Q, you can calculate I^2, which is an estimation of heterogeneity. GREAT. Now what?

# The RMA's above have already done that though! We have estimates of all of that from it. It's just about reporting it, yea?
# Yea.

### TESTING FOR INFLUENTIAL STUDIES

## Where does the heterogeneity come from? Probably from bad p-hacked studies with too few participants and bloated effect sizes (assuming of course there are any bad studies). How do we check for those?

# We check residuals (of what!?):


inf <-influence(rma_f1)
print(inf)
plot(inf)


inf_2 <-influence(rma_f2)
print(inf_2)
plot(inf_2)





### BUT WHAT ABOUT PUBLICATION BIAS!?

## We test for that using Funnel Plots: Plotting effect size against standard error. That is, we check whether the larger the effect size the more unreliable the estimate (otherwise said, whether only bad studies have good results)

funnel(rma_f1, main = "Random-Effects Model", xlab = "Standardized Mean Difference")

## Is a plot enough?

regtest(rma_f1)
ranktest(rma_f1)
# None of these tests show significant assymetry.

### INDEED, WHAT ABOUT PUBLICATION BIAS!?

funnel(rma_f2, main = "Random-Effects Model", xlab = "Standardized Mean Difference")

## Is a plot enough?

regtest(rma_f2)
ranktest(rma_f2)
# It's not not publication bias.


```


```{r}
##### NEED TO DO #####
# Compare with Assignment 3 analysis
# Add data from assignment 3


## Loading packages
pacman::p_load(pacman, readxl, tidyverse, metafor, plyr)



### briefly resetting the working directory!
setwd("C:/Users/Asger/Desktop/Cognitive Science BA/3. Semester/ExpMeth 3/Assignments/Assignment-3/"); all_data <- read.csv("Fully Merged Data.csv"); setwd("C:/Users/Asger/Desktop/Cognitive Science BA/3. Semester/ExpMeth 3/Assignments/Assignment-5")

manageable <- all_data %>%  select(Diagnosis, Study, Trial, Mean, StandardDev, ID)


stuff <- manageable %>% group_by(Study, Diagnosis, ID) %>% dplyr::summarize(f1 = mean(Mean), f2=mean(StandardDev))

# We now have an average variability measure AKA F2 AKA Standard Deviation for each trial, AND an average pitch average measure AKA F1 AKA mean of each trial... averaged.

# And 

stuff_2.0 <- stuff %>% group_by(Study, Diagnosis) %>% dplyr::summarize(f1_mean = mean(f1),f1_sd = sd(f1), f2_mean=mean(f2),f2_sd = sd(f2), sample_size = n())


stuff_2.0

a3_hc <- stuff_2.0 %>% filter(Diagnosis == "Control")
a3_sz <- stuff_2.0 %>% filter(Diagnosis == "Schizophrenia")

a3_hc$Diagnosis <- NULL
a3_sz$Diagnosis <- NULL


colnames(a3_hc) <- c("studyID","f1_hc_mean","f1_hc_sd", "f2_hc_mean", "f2_hc_sd", "n_hc")
colnames(a3_sz) <- c("studyID","f1_sz_mean","f1_sz_sd", "f2_sz_mean", "f2_sz_sd", "n_sz")

a3_data <- cbind(a3_hc, a3_sz)
a3_data[,7] <- NULL

a3_data$studyID <- a3_data$studyID + 9000 # IT'S OVER NINE THOUSAAAAAND

write.csv(a3_data, "Assignment 3 Data.csv")

### Fuckin'. Wonderful.

a3_data <- read.csv("Assignment 3 Data.csv")

a3_f1 <- a3_data %>% select("studyID","n_hc", "n_sz", "f1_hc_mean", "f1_sz_mean", "f1_hc_sd", "f1_sz_sd")

a3_f2 <- a3_data %>% select("studyID", "n_hc", "n_sz", "f2_hc_mean", "f2_sz_mean", "f2_hc_sd", "f2_sz_sd")



df_f1 <- read.csv("F1 Dataframe.csv")
df_f2 <- read.csv("F2 Dataframe.csv")


df_f1 <- df_f1 %>% select("studyID","n_hc", "n_sz", "f1_hc_mean", "f1_sz_mean", "f1_hc_sd", "f1_sz_sd")

df_f2 <- df_f2 %>% select("studyID", "n_hc", "n_sz", "f2_hc_mean", "f2_sz_mean", "f2_hc_sd", "f2_sz_sd")



# Removing columns that aren't in a3_data
all_f1 <- rbind(df_f1, a3_f1)
write.csv(all_f1, "Merged F1 Data.csv")
all_f2 <- rbind(df_f2, a3_f2)
write.csv(all_f2, "Merged F2 Data.csv")

```



```{r}
##### NEED TO DO #####
# Compare with Assignment 3 analysis
# Add data from assignment 3


## Loading packages
pacman::p_load(pacman, readxl, tidyverse, metafor, plyr)

all_f1 <- read.csv("Merged F1 Data.csv")
all_f2 <- read.csv("Merged F2 Data.csv")


es_f1_2 <- escalc('SMD',
                      n1i = n_hc,
                      n2i = n_sz,
                      m1i = f1_hc_mean, 
                      m2i = f1_sz_mean,
                      sd1i = f1_hc_sd, 
                      sd2i = f1_sz_sd,
                      data = all_f1)


# Effect size calculations for feature 2 (Participant's pitch variability)
es_f2_2 <- escalc('SMD',
                      n1i = n_hc,
                      n2i = n_sz,
                      m1i = f2_hc_mean, 
                      m2i = f2_sz_mean,
                      sd1i = f2_hc_sd, 
                      sd2i = f2_sz_sd,
                      data = all_f2)

## Recreating the lmer models
# Thus, the basic pattern becomes:

mod_f1_2 <- lmerTest::lmer(yi ~ 1 + (1 | studyID), es_f1_2, weights = 1/vi, REML=F, control = lme4::lmerControl( check.nobs.vs.nlev ='ignore', check.nobs.vs.nRE = 'ignore'))
summary(mod_f1_2) # We get boundary (singular) fit again. Ouch. Does it matter? WHo knows. Maybe Riccardo does.

# This is the *weighted average effect size* (with heterogeneity taken into account) of the difference in feature 1 (Pitch Mean) between Healthy Controls and Schizophrenics across studies. Phew.

# We may wish to include a fixed effect of type of task, to see if it changes anything:

## Rinse repeat for F2:

mod_f2_2 <- lmerTest::lmer(yi ~ 1 + (1 | studyID), es_f2_2, weights = 1/vi, REML=F, control = lme4::lmerControl( check.nobs.vs.nlev ='ignore', check.nobs.vs.nRE = 'ignore'))
summary(mod_f2_2) # Interestingly, similar results. 

# Once again...... this is the weighted average standardized mean difference (with random effects) of the differences in Standard Deviation between Healthy Controls and Schizophrenics... across studies.

# We may also wish to plot the results. Or maybe not.

plot(mod_f1_2) # I would like some aid in interpreting this.
plot(mod_f2_2) # THis plot is a bit weird with or without the random effects.

### Ask Riccardo for help in interpreting the plots + whether we should report them.

### Now, we can basically do all of the above using metafor's "rma" function.

rma_f1_2 <- rma(yi, vi, data = es_f1_2, slab = studyID)
summary(rma_f1_2) # Gives a somewhat different output. Scaled, perhaps? ### ASK RICCARDO
forest(rma_f1_2) # Nice forest plot of the metaanalysis! ...What does it show again?

#tau^2 (estimated amount of total heterogeneity): 0.0931 (SE = 0.0721)
#tau (square root of estimated tau^2 value):      0.3051
#I^2 (total heterogeneity / total variability):   53.98%
#H^2 (total variability / sampling variability):  2.17

#Test for Heterogeneity:
#Q(df = 12) = 27.1776, p-val = 0.0073

#Model Results:

#estimate      se     zval    pval    ci.lb   ci.ub 
# -0.0569  0.1179  -0.4828  0.6293  -0.2879  0.1741    


# For feature 2
rma_f2_2 <- rma(yi, vi, data = es_f2_2, slab = studyID)
summary(rma_f2_2)

#tau^2 (estimated amount of total heterogeneity): 0.7931 (SE = 0.2718)
#tau (square root of estimated tau^2 value):      0.8906
#I^2 (total heterogeneity / total variability):   92.00%
#H^2 (total variability / sampling variability):  12.50

#Test for Heterogeneity:
#Q(df = 21) = 171.6114, p-val < .0001

#Model Results:

#estimate      se    zval    pval    ci.lb   ci.ub 
#  0.2490  0.2002  1.2434  0.2137  -0.1435  0.6414 


forest(rma_f2_2)# It do be lookethening like there doth be an influential study, yo.15 <- dat one.

# Now with task (again)
rma_task_f2 <-rma(yi, vi, mods = cbind(task), data = es_f2, slab = studyID)
summary(rma_task_f2)
#tau^2 (estimated amount of residual heterogeneity):     1.2275 (SE = 0.5120)
#tau (square root of estimated tau^2 value):             1.1079
#I^2 (residual heterogeneity / unaccounted variability): 95.16%
#H^2 (unaccounted variability / sampling variability):   20.68
#R^2 (amount of heterogeneity accounted for):            0.00%

#Test for Residual Heterogeneity:
#QE(df = 13) = 158.7379, p-val < .0001

#Test of Moderators (coefficient 2):
#QM(df = 1) = 0.7244, p-val = 0.3947


forest(rma_task_f2)


### TESTING FOR INFLUENTIAL STUDIES

## Where does the heterogeneity come from? Probably from bad p-hacked studies with too few participants and bloated effect sizes (assuming of course there are any bad studies). How do we check for those?

# We check residuals (of what!?):


inf_3 <-influence(rma_f1_2)
print(inf_3)
plot(inf_3)


inf_4 <-influence(rma_f2_2)
print(inf_4)
plot(inf_4)





### BUT WHAT ABOUT PUBLICATION BIAS!?

## We test for that using Funnel Plots: Plotting effect size against standard error. That is, we check whether the larger the effect size the more unreliable the estimate (otherwise said, whether only bad studies have good results)

funnel(rma_f1, main = "Random-Effects Model", xlab = "Standardized Mean Difference")

## Is a plot enough?

regtest(rma_f1)
ranktest(rma_f1)
# None of these tests show significant assymetry.

### INDEED, WHAT ABOUT PUBLICATION BIAS!?

funnel(rma_f2, main = "Random-Effects Model", xlab = "Standardized Mean Difference")

## Is a plot enough?

regtest(rma_f2)
ranktest(rma_f2)
# It's not not publication bias.




```

# Reporting heterogeneity

We then calculated overall variance (τ2) and assessed whether it could be explained by within-study variance (e.g., due to measurement noise or heterogeneity in the ASD samples included in the studies) using Cochran’s Q (Cochran, 1954) and I2statistics (Higgins, Thompson, Deeks, & Altman, 2003).
Overall variance (τ2)of 0.18 (95% CIs: 0.04 0.61). Much of the variance (I2: 60.18%, 95% CIs: 26.83 83.38) could not be reduced to random sample variability between studies (Q-stats = 39.94, p = 0.0008).

#What to do? Try out different explanation of variance (as pre-planned):

However, neither task (estimate: 0.2, 95% CIs -0.15 0.55, p=0.27) nor language (estimate: -0.03, 95% CIs -0.12 0.05, p=0.42) could significantly explain the variance




#Reporting publication bias

Finally, we investigated the effect of influential studies (single studies strongly driving the overall results) and publication bias (tendency to write up and publish only significant findings, ignoring null findings and making the literature unrepresentative of the actual population studied) on the robustness of our analysis. This was estimated using rank correlation tests assessing whether lower sample sizes (and relatedly higher standard error) were related to bigger effect sizes. A significant rank correlation indicates a likely publication bias and inflated effect sizes due to small samples. 

There were no obvious outliers, nor any obvious publication bias (Kendall's τ = 0.06, p = 0.78; Figure 4). Indeed, of the 4 studies where statistical estimates were not available, 2 reported null findings and 2 included cases in which participants with ASD presented a wider pitch range, slightly reinforcing the hypothesis of a positive effect size.